{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forests - Redux"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From Fastai ML1 [Lesson 1 Intro to Random Forests](https://github.com/fastai/fastai/blob/master/courses/ml1/lesson1-rf.ipynb)\n",
    "\n",
    "\n",
    "This notebook turned into a redux of my [first RF Code Along](https://github.com/WNoxchi/Kaukasos/blob/master/FAML1/Lesson1-RandomForests.ipynb) with notes.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai.imports import *\n",
    "from fastai.structured import *\n",
    "\n",
    "from pandas_summary import DataFrameSummary\n",
    "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\n",
    "from IPython.display import display\n",
    "\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = \"../../data/competitions/bluebook-for-bulldozers/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data%20Dictionary.xlsx\t\t  Train.7z\t     Valid.7z\r\n",
      "Machine_Appendix.csv\t\t  TrainAndValid.7z   Valid.csv\r\n",
      "median_benchmark.csv\t\t  TrainAndValid.csv  ValidSolution.csv\r\n",
      "random_forest_benchmark_test.csv  TrainAndValid.zip  Valid.zip\r\n",
      "Test.csv\t\t\t  Train.csv\r\n",
      "tmp\t\t\t\t  Train.zip\r\n"
     ]
    }
   ],
   "source": [
    "!ls {PATH}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`low_memory=False` tells Pandas to read more of the file to decide what the types are.\n",
    "\n",
    "`parse_dates=[...]` is used for any columns that contain dates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_raw = pd.read_csv(f'{PATH}Train.csv', low_memory=False, parse_dates=['saledate'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Entering a DataFrame to display it will truncate it if it's too long.\n",
    "This function sets the truncation threshold to 1000 rows & cols."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_all(df):\n",
    "    with pd.option_context(\"display.max_rows\", 1000):\n",
    "        with pd.option_context(\"display.max_columns\", 1000):\n",
    "            display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`df_raw.tail()` will show the last few rows of the DataFrame. By default it shows the \n",
    "cols at top and rows on side. There're a lot of cols, so using `.transpose()` \n",
    "displays the table on its side."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display_all(df_raw.tail().transpose())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# display_all(df_raw.describe(include='all').transpose())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_raw.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[RMSLE](https://www.kaggle.com/c/bluebook-for-bulldozers#evaluation) is used in the Kaggle competition. So by taking the log of all sale prices, we can just use RMSE later to calculate our loss. RMSLE: $Σ\\big(($log(prediction) - log(actual)$)^2\\big)$ : this means ratios not absolutes.\n",
    "\n",
    "Here we also replace a column w/ a new column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_raw.SalePrice = np.log(df_raw.SalePrice)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.2 Initial Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A Random Forest is smth of a universal machine learning technique. Could be a category or a continous variable - and can predict w/ cols of almost any kind (pixel data, zip codes, etc). RFs genrly don't overfit (prevention: easy). RFs don't genrly req validation sets, & can tell you how well they generalize - even w/ only 1 dataset. RFs have few if any statistical assumptions of your data, & req v.few pieces of feature engineering.\n",
    "\n",
    "`model.fit(`__`Independant Variables`__`, `__`Dependent Variables`__`)`\n",
    "\n",
    "Indep: used to predict; Dep: predicted. `pandas.DataFrame.drop(..)` returns a new DataFrame w/ a list of rows/cols removed. So we use everything but the SalePrice to predict the SalePrice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "could not convert string to float: 'Conventional'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-c478fc144154>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRandomForestRegressor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# n_jobs: number of cores to use. -1 ==> all\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_raw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'SalePrice'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf_raw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSalePrice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/src/anaconda3/envs/fastai/lib/python3.6/site-packages/sklearn/ensemble/forest.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    245\u001b[0m         \"\"\"\n\u001b[1;32m    246\u001b[0m         \u001b[0;31m# Validate or convert input data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 247\u001b[0;31m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccept_sparse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"csc\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mDTYPE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    248\u001b[0m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccept_sparse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'csc'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mensure_2d\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    249\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0msample_weight\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/src/anaconda3/envs/fastai/lib/python3.6/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, warn_on_dtype, estimator)\u001b[0m\n\u001b[1;32m    431\u001b[0m                                       force_all_finite)\n\u001b[1;32m    432\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 433\u001b[0;31m         \u001b[0marray\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    434\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    435\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mensure_2d\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: could not convert string to float: 'Conventional'"
     ]
    }
   ],
   "source": [
    "model = RandomForestRegressor(n_jobs=-1) # n_jobs: number of cores to use. -1 ==> all\n",
    "model.fit(df_raw.drop('SalePrice', axis=1), df_raw.SalePrice)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This dataset contains a mix of **continuous** and __categorical__ variables. Most ML models (incl. RFs) req numbers -- so we need to convert all our cols to numbers.\n",
    "\n",
    "\n",
    "`sklearn.ensemble.RandomForestRegressor`: predict __continuous__ variables\n",
    "\n",
    "`sklearn.ensemble.RandomForestClassifier`: predict __categorical__ variables\n",
    "\n",
    "---\n",
    "\n",
    "One issue is `saledate` was parsed as a date $ \\longrightarrow $ as a number. But if we look at it, it isn't a number, it's a `datetime64` -- which is __not__ a number. So we need to do our first bit of feature engineering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0   2006-11-16\n",
       "1   2004-03-26\n",
       "2   2004-02-26\n",
       "3   2011-05-19\n",
       "4   2009-07-23\n",
       "Name: saledate, dtype: datetime64[ns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_raw.saledate[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inside `fastai.structured` is a function called `add_datepart`, which we'll use to fix this.\n",
    "\n",
    "__Overview of `add_datepart`:__\n",
    "\n",
    "1. We pass in a dataframe and a field (in this case `'saledate'`) to `add_datepart(df, fldname)`. We can't do `df.fieldname` because that'd return a field called 'fieldname'. So `df[fldname]` is how we grab a column when that column name is stored in the variable `fldname`. This gives us the field itself, the `pd.Series`.\n",
    "\n",
    "2. `add_datepart` then goes through a list of date attribute strings ('Year', 'Month', 'Dayofyear', etc) and builds new columns by looking them up in `fld`'s datetime attributes (`fld.dt`).\n",
    "\n",
    "3. It finally drops the original `fldname` column (`'saledate'` here) because it isn't numerical.\n",
    "\n",
    "---\n",
    "\n",
    "***NOTE***: `'saledate'` is a date type because we told Pandas to make it such via `parse_dates=[\"saledate\"]`. That's why it has the relevant datetime attributes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    2006\n",
       "1    2004\n",
       "2    2004\n",
       "3    2011\n",
       "4    2009\n",
       "Name: saleYear, dtype: int64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "add_datepart(df_raw, 'saledate')\n",
    "df_raw.saleYear.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the datatype for `'saledate'` is numerical (`int64`). If we check the columns of the DataFrame we'll see the new ones added by `add_datepart`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['SalesID', 'SalePrice', 'MachineID', 'ModelID', 'datasource',\n",
       "       'auctioneerID', 'YearMade', 'MachineHoursCurrentMeter', 'UsageBand',\n",
       "       'fiModelDesc', 'fiBaseModel', 'fiSecondaryDesc', 'fiModelSeries',\n",
       "       'fiModelDescriptor', 'ProductSize', 'fiProductClassDesc', 'state',\n",
       "       'ProductGroup', 'ProductGroupDesc', 'Drive_System', 'Enclosure',\n",
       "       'Forks', 'Pad_Type', 'Ride_Control', 'Stick', 'Transmission',\n",
       "       'Turbocharged', 'Blade_Extension', 'Blade_Width', 'Enclosure_Type',\n",
       "       'Engine_Horsepower', 'Hydraulics', 'Pushblock', 'Ripper', 'Scarifier',\n",
       "       'Tip_Control', 'Tire_Size', 'Coupler', 'Coupler_System',\n",
       "       'Grouser_Tracks', 'Hydraulics_Flow', 'Track_Type',\n",
       "       'Undercarriage_Pad_Width', 'Stick_Length', 'Thumb', 'Pattern_Changer',\n",
       "       'Grouser_Type', 'Backhoe_Mounting', 'Blade_Type', 'Travel_Controls',\n",
       "       'Differential_Type', 'Steering_Controls', 'saleYear', 'saleMonth',\n",
       "       'saleWeek', 'saleDay', 'saleDayofweek', 'saleDayofyear',\n",
       "       'saleIs_month_end', 'saleIs_month_start', 'saleIs_quarter_end',\n",
       "       'saleIs_quarter_start', 'saleIs_year_end', 'saleIs_year_start',\n",
       "       'saleElapsed'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_raw.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This isn't enough. One more bit of feature engineering is needed: there are strings in the dataset (`'Low'`, `'High'`) etc. FastAI has function to automatically create categorical variables for all strings - by creating a column (backend) mapping integers to strings.\n",
    "\n",
    "FastAI also has a `apply_cats` function to preserve training-set category mappings for validation & test set use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SalesID</th>\n",
       "      <th>SalePrice</th>\n",
       "      <th>MachineID</th>\n",
       "      <th>ModelID</th>\n",
       "      <th>datasource</th>\n",
       "      <th>auctioneerID</th>\n",
       "      <th>YearMade</th>\n",
       "      <th>MachineHoursCurrentMeter</th>\n",
       "      <th>UsageBand</th>\n",
       "      <th>fiModelDesc</th>\n",
       "      <th>...</th>\n",
       "      <th>saleDay</th>\n",
       "      <th>saleDayofweek</th>\n",
       "      <th>saleDayofyear</th>\n",
       "      <th>saleIs_month_end</th>\n",
       "      <th>saleIs_month_start</th>\n",
       "      <th>saleIs_quarter_end</th>\n",
       "      <th>saleIs_quarter_start</th>\n",
       "      <th>saleIs_year_end</th>\n",
       "      <th>saleIs_year_start</th>\n",
       "      <th>saleElapsed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1139246</td>\n",
       "      <td>11.097410</td>\n",
       "      <td>999089</td>\n",
       "      <td>3157</td>\n",
       "      <td>121</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2004</td>\n",
       "      <td>68.0</td>\n",
       "      <td>Low</td>\n",
       "      <td>521D</td>\n",
       "      <td>...</td>\n",
       "      <td>16</td>\n",
       "      <td>3</td>\n",
       "      <td>320</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>1163635200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1139248</td>\n",
       "      <td>10.950807</td>\n",
       "      <td>117657</td>\n",
       "      <td>77</td>\n",
       "      <td>121</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1996</td>\n",
       "      <td>4640.0</td>\n",
       "      <td>Low</td>\n",
       "      <td>950FII</td>\n",
       "      <td>...</td>\n",
       "      <td>26</td>\n",
       "      <td>4</td>\n",
       "      <td>86</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>1080259200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1139249</td>\n",
       "      <td>9.210340</td>\n",
       "      <td>434808</td>\n",
       "      <td>7009</td>\n",
       "      <td>121</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2001</td>\n",
       "      <td>2838.0</td>\n",
       "      <td>High</td>\n",
       "      <td>226</td>\n",
       "      <td>...</td>\n",
       "      <td>26</td>\n",
       "      <td>3</td>\n",
       "      <td>57</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>1077753600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1139251</td>\n",
       "      <td>10.558414</td>\n",
       "      <td>1026470</td>\n",
       "      <td>332</td>\n",
       "      <td>121</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2001</td>\n",
       "      <td>3486.0</td>\n",
       "      <td>High</td>\n",
       "      <td>PC120-6E</td>\n",
       "      <td>...</td>\n",
       "      <td>19</td>\n",
       "      <td>3</td>\n",
       "      <td>139</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>1305763200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1139253</td>\n",
       "      <td>9.305651</td>\n",
       "      <td>1057373</td>\n",
       "      <td>17311</td>\n",
       "      <td>121</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2007</td>\n",
       "      <td>722.0</td>\n",
       "      <td>Medium</td>\n",
       "      <td>S175</td>\n",
       "      <td>...</td>\n",
       "      <td>23</td>\n",
       "      <td>3</td>\n",
       "      <td>204</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>1248307200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 65 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   SalesID  SalePrice  MachineID  ModelID  datasource  auctioneerID  YearMade  \\\n",
       "0  1139246  11.097410     999089     3157         121           3.0      2004   \n",
       "1  1139248  10.950807     117657       77         121           3.0      1996   \n",
       "2  1139249   9.210340     434808     7009         121           3.0      2001   \n",
       "3  1139251  10.558414    1026470      332         121           3.0      2001   \n",
       "4  1139253   9.305651    1057373    17311         121           3.0      2007   \n",
       "\n",
       "   MachineHoursCurrentMeter UsageBand fiModelDesc     ...     saleDay  \\\n",
       "0                      68.0       Low        521D     ...          16   \n",
       "1                    4640.0       Low      950FII     ...          26   \n",
       "2                    2838.0      High         226     ...          26   \n",
       "3                    3486.0      High    PC120-6E     ...          19   \n",
       "4                     722.0    Medium        S175     ...          23   \n",
       "\n",
       "  saleDayofweek saleDayofyear saleIs_month_end saleIs_month_start  \\\n",
       "0             3           320            False              False   \n",
       "1             4            86            False              False   \n",
       "2             3            57            False              False   \n",
       "3             3           139            False              False   \n",
       "4             3           204            False              False   \n",
       "\n",
       "  saleIs_quarter_end saleIs_quarter_start saleIs_year_end saleIs_year_start  \\\n",
       "0              False                False           False             False   \n",
       "1              False                False           False             False   \n",
       "2              False                False           False             False   \n",
       "3              False                False           False             False   \n",
       "4              False                False           False             False   \n",
       "\n",
       "  saleElapsed  \n",
       "0  1163635200  \n",
       "1  1080259200  \n",
       "2  1077753600  \n",
       "3  1305763200  \n",
       "4  1248307200  \n",
       "\n",
       "[5 rows x 65 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_raw.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_cats(df_raw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can access categorical variables as `.cat`attributes just as we could with `.dt` for datetime:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['High', 'Low', 'Medium'], dtype='object')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_raw.UsageBand.cat.categories"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "'High', 'Low', 'Medium' in `UsageBand` will be seen by the RF as cats `0`, `1`, `2`. It'll form a split first on either `0` vs `1, 2`, or `2` vs `0, 1`. That translates to 'High' vs 'Low' & 'Medium' or 'Medium' vs 'High' & 'Low'. That's a bit odd, and though the DT can get to a correct split regardless, by using a sensible ordering we can ensure it gets there in fewer splits - thus improving our model.\n",
    "\n",
    "So we reorder 'High', 'Low', 'Medium' st. they're ordered wrt the category numbers, ie: so that any split starts by comparing 'High' and 'Low':\n",
    "\n",
    "'High','Medium','Low' $\\longrightarrow$ 0, 1, 2\n",
    "\n",
    "`ordered=True` preserved supplied order, `inplace=True` changes the DataFrame in place instead of returning a new one.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_raw.UsageBand.cat.set_categories(['High','Medium','Low'], ordered=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100    Medium\n",
      "101    Medium\n",
      "102    Medium\n",
      "103      High\n",
      "104       NaN\n",
      "105       Low\n",
      "106       Low\n",
      "107      High\n",
      "108    Medium\n",
      "109    Medium\n",
      "Name: UsageBand, dtype: category\n",
      "Categories (3, object): [High < Medium < Low]\n",
      "100    1\n",
      "101    1\n",
      "102    1\n",
      "103    0\n",
      "104   -1\n",
      "105    2\n",
      "106    2\n",
      "107    0\n",
      "108    1\n",
      "109    1\n",
      "dtype: int8\n"
     ]
    }
   ],
   "source": [
    "print(df_raw.UsageBand[100:110])\n",
    "print(df_raw.UsageBand.cat.codes[100:110])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.3 Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We still have a number of Null values. Here we display the fraction of Null values in each category:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Backhoe_Mounting            0.803872\n",
       "Blade_Extension             0.937129\n",
       "Blade_Type                  0.800977\n",
       "Blade_Width                 0.937129\n",
       "Coupler                     0.466620\n",
       "Coupler_System              0.891660\n",
       "Differential_Type           0.826959\n",
       "Drive_System                0.739829\n",
       "Enclosure                   0.000810\n",
       "Enclosure_Type              0.937129\n",
       "Engine_Horsepower           0.937129\n",
       "Forks                       0.521154\n",
       "Grouser_Tracks              0.891899\n",
       "Grouser_Type                0.752813\n",
       "Hydraulics                  0.200823\n",
       "Hydraulics_Flow             0.891899\n",
       "MachineHoursCurrentMeter    0.644089\n",
       "MachineID                   0.000000\n",
       "ModelID                     0.000000\n",
       "Pad_Type                    0.802720\n",
       "Pattern_Changer             0.752651\n",
       "ProductGroup                0.000000\n",
       "ProductGroupDesc            0.000000\n",
       "ProductSize                 0.525460\n",
       "Pushblock                   0.937129\n",
       "Ride_Control                0.629527\n",
       "Ripper                      0.740388\n",
       "SalePrice                   0.000000\n",
       "SalesID                     0.000000\n",
       "Scarifier                   0.937102\n",
       "Steering_Controls           0.827064\n",
       "Stick                       0.802720\n",
       "Stick_Length                0.752651\n",
       "Thumb                       0.752476\n",
       "Tip_Control                 0.937129\n",
       "Tire_Size                   0.763869\n",
       "Track_Type                  0.752813\n",
       "Transmission                0.543210\n",
       "Travel_Controls             0.800975\n",
       "Turbocharged                0.802720\n",
       "Undercarriage_Pad_Width     0.751020\n",
       "UsageBand                   0.826391\n",
       "YearMade                    0.000000\n",
       "auctioneerID                0.050199\n",
       "datasource                  0.000000\n",
       "fiBaseModel                 0.000000\n",
       "fiModelDesc                 0.000000\n",
       "fiModelDescriptor           0.820707\n",
       "fiModelSeries               0.858129\n",
       "fiProductClassDesc          0.000000\n",
       "fiSecondaryDesc             0.342016\n",
       "saleDay                     0.000000\n",
       "saleDayofweek               0.000000\n",
       "saleDayofyear               0.000000\n",
       "saleElapsed                 0.000000\n",
       "saleIs_month_end            0.000000\n",
       "saleIs_month_start          0.000000\n",
       "saleIs_quarter_end          0.000000\n",
       "saleIs_quarter_start        0.000000\n",
       "saleIs_year_end             0.000000\n",
       "saleIs_year_start           0.000000\n",
       "saleMonth                   0.000000\n",
       "saleWeek                    0.000000\n",
       "saleYear                    0.000000\n",
       "state                       0.000000\n",
       "dtype: float64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display_all(df_raw.isnull().sum().sort_index()/len(df_raw))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we'll save our dataframe to disk in feather format since we have a good starting point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(PATH + 'tmp', exist_ok=True)\n",
    "df_raw.to_feather(PATH + 'tmp/raw')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we want to replace the string categories with their numeric codes, handle missing continous values, and pull out the dependant variable (`SalePrice`) into a separate variable. The `fastai.structured.proc_df` is what we'll use to do this.\n",
    "\n",
    "---\n",
    "\n",
    "**Overview of `proc_df`:**\n",
    "\n",
    "`df:` DataFrame | `y_fld`: name of dependent variable\n",
    "\n",
    "• Makes copy of DataFrame. • Grabs y values. • Drops DepVar from DataFrame. • Then fixes missing via `fastai.structured.fix_missing`.\n",
    "\n",
    ">**Overview of `fix_missing`:**\n",
    ">\n",
    ">• Check that the column has missing values (`pd.isnull(col).sum() != 0`). • Create new column with same name as original + '_na' that's a boolean column w/ **1** any time a value is missing, **0** otherwise. • Then replace all Null values with the columns median.\n",
    ">\n",
    ">ie: All NaNs replaced by col's median. New col added keeping track of NaNs.\n",
    "\n",
    "That is done for numeric variables (cols) -- Pandas automatically handles categorical variables by setting them to `-1` if missing.\n",
    "\n",
    "• Then call `fastai.structured.numericalize`.\n",
    "\n",
    ">**Overview of `numericalize`:**\n",
    ">\n",
    ">• If column is **Not** numeric and **is** a categorical type: replace column w/ it's category codes (integers) + 1.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "df, y, nans = proc_df(df_raw, 'SalePrice')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "'SalePrice' is now absent from the DataFrame's columns, and all columns with a non-zero value for null-fractions have corresponding '_na' columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['SalesID', 'MachineID', 'ModelID', 'datasource', 'auctioneerID',\n",
       "       'YearMade', 'MachineHoursCurrentMeter', 'UsageBand', 'fiModelDesc',\n",
       "       'fiBaseModel', 'fiSecondaryDesc', 'fiModelSeries', 'fiModelDescriptor',\n",
       "       'ProductSize', 'fiProductClassDesc', 'state', 'ProductGroup',\n",
       "       'ProductGroupDesc', 'Drive_System', 'Enclosure', 'Forks', 'Pad_Type',\n",
       "       'Ride_Control', 'Stick', 'Transmission', 'Turbocharged',\n",
       "       'Blade_Extension', 'Blade_Width', 'Enclosure_Type', 'Engine_Horsepower',\n",
       "       'Hydraulics', 'Pushblock', 'Ripper', 'Scarifier', 'Tip_Control',\n",
       "       'Tire_Size', 'Coupler', 'Coupler_System', 'Grouser_Tracks',\n",
       "       'Hydraulics_Flow', 'Track_Type', 'Undercarriage_Pad_Width',\n",
       "       'Stick_Length', 'Thumb', 'Pattern_Changer', 'Grouser_Type',\n",
       "       'Backhoe_Mounting', 'Blade_Type', 'Travel_Controls',\n",
       "       'Differential_Type', 'Steering_Controls', 'saleYear', 'saleMonth',\n",
       "       'saleWeek', 'saleDay', 'saleDayofweek', 'saleDayofyear',\n",
       "       'saleIs_month_end', 'saleIs_month_start', 'saleIs_quarter_end',\n",
       "       'saleIs_quarter_start', 'saleIs_year_end', 'saleIs_year_start',\n",
       "       'saleElapsed', 'auctioneerID_na', 'MachineHoursCurrentMeter_na'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we check the DataFrame, we see that everything is now a number:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SalesID</th>\n",
       "      <th>MachineID</th>\n",
       "      <th>ModelID</th>\n",
       "      <th>datasource</th>\n",
       "      <th>auctioneerID</th>\n",
       "      <th>YearMade</th>\n",
       "      <th>MachineHoursCurrentMeter</th>\n",
       "      <th>UsageBand</th>\n",
       "      <th>fiModelDesc</th>\n",
       "      <th>fiBaseModel</th>\n",
       "      <th>...</th>\n",
       "      <th>saleDayofyear</th>\n",
       "      <th>saleIs_month_end</th>\n",
       "      <th>saleIs_month_start</th>\n",
       "      <th>saleIs_quarter_end</th>\n",
       "      <th>saleIs_quarter_start</th>\n",
       "      <th>saleIs_year_end</th>\n",
       "      <th>saleIs_year_start</th>\n",
       "      <th>saleElapsed</th>\n",
       "      <th>auctioneerID_na</th>\n",
       "      <th>MachineHoursCurrentMeter_na</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1139246</td>\n",
       "      <td>999089</td>\n",
       "      <td>3157</td>\n",
       "      <td>121</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2004</td>\n",
       "      <td>68.0</td>\n",
       "      <td>3</td>\n",
       "      <td>950</td>\n",
       "      <td>296</td>\n",
       "      <td>...</td>\n",
       "      <td>320</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>1163635200</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1139248</td>\n",
       "      <td>117657</td>\n",
       "      <td>77</td>\n",
       "      <td>121</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1996</td>\n",
       "      <td>4640.0</td>\n",
       "      <td>3</td>\n",
       "      <td>1725</td>\n",
       "      <td>527</td>\n",
       "      <td>...</td>\n",
       "      <td>86</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>1080259200</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1139249</td>\n",
       "      <td>434808</td>\n",
       "      <td>7009</td>\n",
       "      <td>121</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2001</td>\n",
       "      <td>2838.0</td>\n",
       "      <td>1</td>\n",
       "      <td>331</td>\n",
       "      <td>110</td>\n",
       "      <td>...</td>\n",
       "      <td>57</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>1077753600</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1139251</td>\n",
       "      <td>1026470</td>\n",
       "      <td>332</td>\n",
       "      <td>121</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2001</td>\n",
       "      <td>3486.0</td>\n",
       "      <td>1</td>\n",
       "      <td>3674</td>\n",
       "      <td>1375</td>\n",
       "      <td>...</td>\n",
       "      <td>139</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>1305763200</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1139253</td>\n",
       "      <td>1057373</td>\n",
       "      <td>17311</td>\n",
       "      <td>121</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2007</td>\n",
       "      <td>722.0</td>\n",
       "      <td>2</td>\n",
       "      <td>4208</td>\n",
       "      <td>1529</td>\n",
       "      <td>...</td>\n",
       "      <td>204</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>1248307200</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 66 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   SalesID  MachineID  ModelID  datasource  auctioneerID  YearMade  \\\n",
       "0  1139246     999089     3157         121           3.0      2004   \n",
       "1  1139248     117657       77         121           3.0      1996   \n",
       "2  1139249     434808     7009         121           3.0      2001   \n",
       "3  1139251    1026470      332         121           3.0      2001   \n",
       "4  1139253    1057373    17311         121           3.0      2007   \n",
       "\n",
       "   MachineHoursCurrentMeter  UsageBand  fiModelDesc  fiBaseModel  \\\n",
       "0                      68.0          3          950          296   \n",
       "1                    4640.0          3         1725          527   \n",
       "2                    2838.0          1          331          110   \n",
       "3                    3486.0          1         3674         1375   \n",
       "4                     722.0          2         4208         1529   \n",
       "\n",
       "              ...               saleDayofyear  saleIs_month_end  \\\n",
       "0             ...                         320             False   \n",
       "1             ...                          86             False   \n",
       "2             ...                          57             False   \n",
       "3             ...                         139             False   \n",
       "4             ...                         204             False   \n",
       "\n",
       "   saleIs_month_start  saleIs_quarter_end  saleIs_quarter_start  \\\n",
       "0               False               False                 False   \n",
       "1               False               False                 False   \n",
       "2               False               False                 False   \n",
       "3               False               False                 False   \n",
       "4               False               False                 False   \n",
       "\n",
       "   saleIs_year_end  saleIs_year_start  saleElapsed  auctioneerID_na  \\\n",
       "0            False              False   1163635200            False   \n",
       "1            False              False   1080259200            False   \n",
       "2            False              False   1077753600            False   \n",
       "3            False              False   1305763200            False   \n",
       "4            False              False   1248307200            False   \n",
       "\n",
       "   MachineHoursCurrentMeter_na  \n",
       "0                        False  \n",
       "1                        False  \n",
       "2                        False  \n",
       "3                        False  \n",
       "4                        False  \n",
       "\n",
       "[5 rows x 66 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have something we can pass into a Random-Forest Regressor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9831855416872145"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = RandomForestRegressor(n_jobs=-1)\n",
    "model.fit(df, y)\n",
    "model.score(df, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***NOTE***: Random Forests are *trivially* parallelizable, meaning computation time more/less linearly scales (negatively) with number of CPUs.\n",
    "\n",
    "The score is the R$^2$ value. Range is < 1. 1 is perfect. If your R$^2$ score is < 0 your model is worse than predicting the mean. [FastAI ML1 L2 bit on R2](https://youtu.be/blyXCk4sgEg?t=718). **Gist of R$^2$:** *the ratio between how good your model is (RMSE) vs. how good is the naïve mean model (RMSE)*.\n",
    "\n",
    "We'll create a simple validation set to test this. The dataset is sorted by date, so the most recent `n` rows will make up the validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((389125, 66), (389125,), (12000, 66))"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def split_vals(a, n): return a[:n].copy(), a[n:].copy()\n",
    "\n",
    "n_valid = 12000 # same as Kaggle's test set size # 12000 rows for val set\n",
    "n_trn = len(df) - n_valid                        # all else in trn set\n",
    "raw_train, raw_valid = split_vals(df_raw, n_trn)\n",
    "X_train, X_valid = split_vals(df, n_trn)\n",
    "y_train, y_valid = split_vals(y, n_trn)\n",
    "\n",
    "X_train.shape, y_train.shape, X_valid.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Random Forests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Base Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll run our model again, but with the separate training and validation sets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rmse(x,y): return math.sqrt(((x-y)**2).mean())\n",
    "\n",
    "def print_score(m):\n",
    "    res = [rmse(m.predict(X_train), y_train), rmse(m.predict(X_valid), y_valid), \n",
    "           m.score(X_train, y_train), m.score(X_valid, y_valid)]\n",
    "    if hasattr(m, 'oob_score_'): res.append(m.oob_score_)\n",
    "    print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 34s, sys: 260 ms, total: 1min 34s\n",
      "Wall time: 29.3 s\n",
      "[0.090431376571641, 0.247444226266789, 0.9829087782181651, 0.8906540698336106]\n"
     ]
    }
   ],
   "source": [
    "model = RandomForestRegressor(n_jobs=-1)\n",
    "%time model.fit(X_train, y_train)\n",
    "print_score(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There was some overfitting going on -- but this 0.252 loss gets into the top 25% of the Kaggle public leaderboard.\n",
    "\n",
    "---\n",
    "[Fast.ai ML1 L2](https://youtu.be/blyXCk4sgEg?t=1114) p.much picks up from here.\n",
    "\n",
    "Since the data/competition is predicted time-series data -- you want your validation set to reflect that by being a range of consecutive dates (specifically some tail slice of the dataset)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Speeding things up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One way to speed up iteration time for model development is to use the `subset` parameter in `proc_df`: which returns only a subset of the data to work on. This returns a randomly sampled subset of the data.\n",
    "\n",
    "We need to make sure our train-subset doesn't overlap with our validation set. We also want to use our original val set, and **not** overwrite it, so of the 30k subset, we set the first 20k (this may overlap a bit..) to be training, and throw the rest away.\n",
    "\n",
    "* Create `df_trn`, `y_trn` from a random 30k subset of `df_raw`. \n",
    "* Set `X_train`, `y_train` to be the first 20k rows of `df_trn`, `y_trn`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_trn, y_trn, nans = proc_df(df_raw, 'SalePrice', subset=30000)\n",
    "X_train, _ = split_vals(df_trn, 20000)\n",
    "y_train, _ = split_vals(y_trn,  20000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3.81 s, sys: 4 ms, total: 3.82 s\n",
      "Wall time: 1.27 s\n",
      "[0.11279606685210727, 0.37086225223751285, 0.9718434771785658, 0.7543746215442799]\n"
     ]
    }
   ],
   "source": [
    "model = RandomForestRegressor(n_jobs=-1) ## initialize Model\n",
    "%time model.fit(X_train, y_train)        ## train Model\n",
    "print_score(model)                       ## run predictions - still using orig valset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loss Train, | Loss Valid, | R2 Train, | R2 Loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Single Tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scikit-Learn calls trees estimators. `max_depth` is depth of splits. `bootstrap` controls random:on/off for Random Forest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.525145713138213, 0.5808629267861516, 0.38968961849576, 0.39744694659014135]\n"
     ]
    }
   ],
   "source": [
    "# A small Deterministic Decision Tree\n",
    "model = RandomForestRegressor(n_estimators=1, max_depth=3, bootstrap=False, n_jobs=-1)\n",
    "model.fit(X_train, y_train)\n",
    "print_score(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`fastai.structured.draw_tree` lets us visualize Decision Trees. `model.estimators_[0]` returns the 1st estimator from an array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.estimators_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       " \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<!-- Generated by graphviz version 2.38.0 (20140413.2041)\n",
       " -->\n",
       "<!-- Title: Tree Pages: 1 -->\n",
       "<svg width=\"720pt\" height=\"434pt\"\n",
       " viewBox=\"0.00 0.00 720.00 434.49\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(0.778659 0.778659) rotate(0) translate(4 554)\">\n",
       "<title>Tree</title>\n",
       "<polygon fill=\"white\" stroke=\"none\" points=\"-4,4 -4,-554 920.667,-554 920.667,4 -4,4\"/>\n",
       "<!-- 0 -->\n",
       "<g id=\"node1\" class=\"node\"><title>0</title>\n",
       "<polygon fill=\"#e58139\" fill-opacity=\"0.756863\" stroke=\"black\" points=\"169.667,-308.5 29.6667,-308.5 29.6667,-240.5 169.667,-240.5 169.667,-308.5\"/>\n",
       "<text text-anchor=\"start\" x=\"37.6667\" y=\"-293.3\" font-family=\"Times,serif\" font-size=\"14.00\">Coupler_System ≤ 0.5</text>\n",
       "<text text-anchor=\"start\" x=\"64.6667\" y=\"-278.3\" font-family=\"Times,serif\" font-size=\"14.00\">mse = 0.452</text>\n",
       "<text text-anchor=\"start\" x=\"52.6667\" y=\"-263.3\" font-family=\"Times,serif\" font-size=\"14.00\">samples = 20000</text>\n",
       "<text text-anchor=\"start\" x=\"61.6667\" y=\"-248.3\" font-family=\"Times,serif\" font-size=\"14.00\">value = 10.12</text>\n",
       "</g>\n",
       "<!-- 1 -->\n",
       "<g id=\"node2\" class=\"node\"><title>1</title>\n",
       "<polygon fill=\"#e58139\" fill-opacity=\"0.807843\" stroke=\"black\" points=\"405.667,-361.5 277.667,-361.5 277.667,-293.5 405.667,-293.5 405.667,-361.5\"/>\n",
       "<text text-anchor=\"start\" x=\"285.667\" y=\"-346.3\" font-family=\"Times,serif\" font-size=\"14.00\">YearMade ≤ 1985.5</text>\n",
       "<text text-anchor=\"start\" x=\"306.667\" y=\"-331.3\" font-family=\"Times,serif\" font-size=\"14.00\">mse = 0.397</text>\n",
       "<text text-anchor=\"start\" x=\"294.667\" y=\"-316.3\" font-family=\"Times,serif\" font-size=\"14.00\">samples = 18370</text>\n",
       "<text text-anchor=\"start\" x=\"300.167\" y=\"-301.3\" font-family=\"Times,serif\" font-size=\"14.00\">value = 10.203</text>\n",
       "</g>\n",
       "<!-- 0&#45;&gt;1 -->\n",
       "<g id=\"edge1\" class=\"edge\"><title>0&#45;&gt;1</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M169.763,-289.759C200.377,-296.519 236.416,-304.478 267.489,-311.34\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"266.87,-314.787 277.389,-313.526 268.379,-307.952 266.87,-314.787\"/>\n",
       "<text text-anchor=\"middle\" x=\"256.316\" y=\"-323.276\" font-family=\"Times,serif\" font-size=\"14.00\">True</text>\n",
       "</g>\n",
       "<!-- 8 -->\n",
       "<g id=\"node9\" class=\"node\"><title>8</title>\n",
       "<polygon fill=\"#e58139\" fill-opacity=\"0.168627\" stroke=\"black\" points=\"405.667,-255.5 277.667,-255.5 277.667,-187.5 405.667,-187.5 405.667,-255.5\"/>\n",
       "<text text-anchor=\"start\" x=\"285.667\" y=\"-240.3\" font-family=\"Times,serif\" font-size=\"14.00\">YearMade ≤ 1998.5</text>\n",
       "<text text-anchor=\"start\" x=\"306.667\" y=\"-225.3\" font-family=\"Times,serif\" font-size=\"14.00\">mse = 0.114</text>\n",
       "<text text-anchor=\"start\" x=\"297.667\" y=\"-210.3\" font-family=\"Times,serif\" font-size=\"14.00\">samples = 1630</text>\n",
       "<text text-anchor=\"start\" x=\"303.667\" y=\"-195.3\" font-family=\"Times,serif\" font-size=\"14.00\">value = 9.182</text>\n",
       "</g>\n",
       "<!-- 0&#45;&gt;8 -->\n",
       "<g id=\"edge8\" class=\"edge\"><title>0&#45;&gt;8</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M169.763,-259.241C200.377,-252.481 236.416,-244.522 267.489,-237.66\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"268.379,-241.048 277.389,-235.474 266.87,-234.213 268.379,-241.048\"/>\n",
       "<text text-anchor=\"middle\" x=\"256.316\" y=\"-218.324\" font-family=\"Times,serif\" font-size=\"14.00\">False</text>\n",
       "</g>\n",
       "<!-- 2 -->\n",
       "<g id=\"node3\" class=\"node\"><title>2</title>\n",
       "<polygon fill=\"#e58139\" fill-opacity=\"0.615686\" stroke=\"black\" points=\"660.667,-486.5 540.667,-486.5 540.667,-418.5 660.667,-418.5 660.667,-486.5\"/>\n",
       "<text text-anchor=\"start\" x=\"548.667\" y=\"-471.3\" font-family=\"Times,serif\" font-size=\"14.00\">ModelID ≤ 4178.5</text>\n",
       "<text text-anchor=\"start\" x=\"565.667\" y=\"-456.3\" font-family=\"Times,serif\" font-size=\"14.00\">mse = 0.333</text>\n",
       "<text text-anchor=\"start\" x=\"556.667\" y=\"-441.3\" font-family=\"Times,serif\" font-size=\"14.00\">samples = 5818</text>\n",
       "<text text-anchor=\"start\" x=\"562.667\" y=\"-426.3\" font-family=\"Times,serif\" font-size=\"14.00\">value = 9.894</text>\n",
       "</g>\n",
       "<!-- 1&#45;&gt;2 -->\n",
       "<g id=\"edge2\" class=\"edge\"><title>1&#45;&gt;2</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M405.865,-358.239C443.841,-376.709 492.18,-400.22 531.08,-419.141\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"529.721,-422.372 540.244,-423.598 532.782,-416.077 529.721,-422.372\"/>\n",
       "</g>\n",
       "<!-- 5 -->\n",
       "<g id=\"node6\" class=\"node\"><title>5</title>\n",
       "<polygon fill=\"#e58139\" fill-opacity=\"0.898039\" stroke=\"black\" points=\"678.667,-361.5 522.667,-361.5 522.667,-293.5 678.667,-293.5 678.667,-361.5\"/>\n",
       "<text text-anchor=\"start\" x=\"530.667\" y=\"-346.3\" font-family=\"Times,serif\" font-size=\"14.00\">fiProductClassDesc ≤ 6.5</text>\n",
       "<text text-anchor=\"start\" x=\"565.667\" y=\"-331.3\" font-family=\"Times,serif\" font-size=\"14.00\">mse = 0.362</text>\n",
       "<text text-anchor=\"start\" x=\"553.667\" y=\"-316.3\" font-family=\"Times,serif\" font-size=\"14.00\">samples = 12552</text>\n",
       "<text text-anchor=\"start\" x=\"559.167\" y=\"-301.3\" font-family=\"Times,serif\" font-size=\"14.00\">value = 10.346</text>\n",
       "</g>\n",
       "<!-- 1&#45;&gt;5 -->\n",
       "<g id=\"edge5\" class=\"edge\"><title>1&#45;&gt;5</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M405.865,-327.5C437.924,-327.5 477.369,-327.5 512.286,-327.5\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"512.483,-331 522.483,-327.5 512.483,-324 512.483,-331\"/>\n",
       "</g>\n",
       "<!-- 3 -->\n",
       "<g id=\"node4\" class=\"node\"><title>3</title>\n",
       "<polygon fill=\"#e58139\" fill-opacity=\"0.749020\" stroke=\"black\" points=\"894.667,-550 790.667,-550 790.667,-497 894.667,-497 894.667,-550\"/>\n",
       "<text text-anchor=\"start\" x=\"807.667\" y=\"-534.8\" font-family=\"Times,serif\" font-size=\"14.00\">mse = 0.361</text>\n",
       "<text text-anchor=\"start\" x=\"798.667\" y=\"-519.8\" font-family=\"Times,serif\" font-size=\"14.00\">samples = 2840</text>\n",
       "<text text-anchor=\"start\" x=\"801.167\" y=\"-504.8\" font-family=\"Times,serif\" font-size=\"14.00\">value = 10.106</text>\n",
       "</g>\n",
       "<!-- 2&#45;&gt;3 -->\n",
       "<g id=\"edge3\" class=\"edge\"><title>2&#45;&gt;3</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M660.685,-469.959C697.251,-480.777 744.105,-494.638 780.978,-505.546\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"780.028,-508.915 790.61,-508.396 782.014,-502.203 780.028,-508.915\"/>\n",
       "</g>\n",
       "<!-- 4 -->\n",
       "<g id=\"node5\" class=\"node\"><title>4</title>\n",
       "<polygon fill=\"#e58139\" fill-opacity=\"0.490196\" stroke=\"black\" points=\"894.667,-479 790.667,-479 790.667,-426 894.667,-426 894.667,-479\"/>\n",
       "<text text-anchor=\"start\" x=\"807.667\" y=\"-463.8\" font-family=\"Times,serif\" font-size=\"14.00\">mse = 0.223</text>\n",
       "<text text-anchor=\"start\" x=\"798.667\" y=\"-448.8\" font-family=\"Times,serif\" font-size=\"14.00\">samples = 2978</text>\n",
       "<text text-anchor=\"start\" x=\"804.667\" y=\"-433.8\" font-family=\"Times,serif\" font-size=\"14.00\">value = 9.692</text>\n",
       "</g>\n",
       "<!-- 2&#45;&gt;4 -->\n",
       "<g id=\"edge4\" class=\"edge\"><title>2&#45;&gt;4</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M660.685,-452.5C697.095,-452.5 743.707,-452.5 780.507,-452.5\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"780.61,-456 790.61,-452.5 780.61,-449 780.61,-456\"/>\n",
       "</g>\n",
       "<!-- 6 -->\n",
       "<g id=\"node7\" class=\"node\"><title>6</title>\n",
       "<polygon fill=\"#e58139\" fill-opacity=\"0.639216\" stroke=\"black\" points=\"894.667,-408 790.667,-408 790.667,-355 894.667,-355 894.667,-408\"/>\n",
       "<text text-anchor=\"start\" x=\"807.667\" y=\"-392.8\" font-family=\"Times,serif\" font-size=\"14.00\">mse = 0.123</text>\n",
       "<text text-anchor=\"start\" x=\"798.667\" y=\"-377.8\" font-family=\"Times,serif\" font-size=\"14.00\">samples = 3566</text>\n",
       "<text text-anchor=\"start\" x=\"804.667\" y=\"-362.8\" font-family=\"Times,serif\" font-size=\"14.00\">value = 9.934</text>\n",
       "</g>\n",
       "<!-- 5&#45;&gt;6 -->\n",
       "<g id=\"edge6\" class=\"edge\"><title>5&#45;&gt;6</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M678.925,-344.883C711.816,-352.284 749.634,-360.793 780.531,-367.745\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"779.78,-371.163 790.305,-369.944 781.317,-364.334 779.78,-371.163\"/>\n",
       "</g>\n",
       "<!-- 7 -->\n",
       "<g id=\"node8\" class=\"node\"><title>7</title>\n",
       "<polygon fill=\"#e58139\" stroke=\"black\" points=\"894.667,-337 790.667,-337 790.667,-284 894.667,-284 894.667,-337\"/>\n",
       "<text text-anchor=\"start\" x=\"807.667\" y=\"-321.8\" font-family=\"Times,serif\" font-size=\"14.00\">mse = 0.363</text>\n",
       "<text text-anchor=\"start\" x=\"798.667\" y=\"-306.8\" font-family=\"Times,serif\" font-size=\"14.00\">samples = 8986</text>\n",
       "<text text-anchor=\"start\" x=\"801.167\" y=\"-291.8\" font-family=\"Times,serif\" font-size=\"14.00\">value = 10.509</text>\n",
       "</g>\n",
       "<!-- 5&#45;&gt;7 -->\n",
       "<g id=\"edge7\" class=\"edge\"><title>5&#45;&gt;7</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M678.925,-322.028C711.673,-319.708 749.307,-317.042 780.13,-314.859\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"780.577,-318.336 790.305,-314.138 780.083,-311.354 780.577,-318.336\"/>\n",
       "</g>\n",
       "<!-- 9 -->\n",
       "<g id=\"node10\" class=\"node\"><title>9</title>\n",
       "<polygon fill=\"#e58139\" fill-opacity=\"0.078431\" stroke=\"black\" points=\"682.167,-255.5 519.167,-255.5 519.167,-187.5 682.167,-187.5 682.167,-255.5\"/>\n",
       "<text text-anchor=\"start\" x=\"527.167\" y=\"-240.3\" font-family=\"Times,serif\" font-size=\"14.00\">fiProductClassDesc ≤ 38.5</text>\n",
       "<text text-anchor=\"start\" x=\"565.667\" y=\"-225.3\" font-family=\"Times,serif\" font-size=\"14.00\">mse = 0.091</text>\n",
       "<text text-anchor=\"start\" x=\"560.167\" y=\"-210.3\" font-family=\"Times,serif\" font-size=\"14.00\">samples = 903</text>\n",
       "<text text-anchor=\"start\" x=\"562.667\" y=\"-195.3\" font-family=\"Times,serif\" font-size=\"14.00\">value = 9.035</text>\n",
       "</g>\n",
       "<!-- 8&#45;&gt;9 -->\n",
       "<g id=\"edge9\" class=\"edge\"><title>8&#45;&gt;9</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M405.865,-221.5C436.844,-221.5 474.719,-221.5 508.741,-221.5\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"509.164,-225 519.164,-221.5 509.164,-218 509.164,-225\"/>\n",
       "</g>\n",
       "<!-- 12 -->\n",
       "<g id=\"node13\" class=\"node\"><title>12</title>\n",
       "<polygon fill=\"#e58139\" fill-opacity=\"0.282353\" stroke=\"black\" points=\"676.667,-131.5 524.667,-131.5 524.667,-63.5 676.667,-63.5 676.667,-131.5\"/>\n",
       "<text text-anchor=\"start\" x=\"532.667\" y=\"-116.3\" font-family=\"Times,serif\" font-size=\"14.00\">MachineID ≤ 1003790.0</text>\n",
       "<text text-anchor=\"start\" x=\"565.667\" y=\"-101.3\" font-family=\"Times,serif\" font-size=\"14.00\">mse = 0.082</text>\n",
       "<text text-anchor=\"start\" x=\"560.167\" y=\"-86.3\" font-family=\"Times,serif\" font-size=\"14.00\">samples = 727</text>\n",
       "<text text-anchor=\"start\" x=\"562.667\" y=\"-71.3\" font-family=\"Times,serif\" font-size=\"14.00\">value = 9.365</text>\n",
       "</g>\n",
       "<!-- 8&#45;&gt;12 -->\n",
       "<g id=\"edge12\" class=\"edge\"><title>8&#45;&gt;12</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M405.865,-191.007C440.255,-174.414 483.144,-153.721 519.826,-136.022\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"521.524,-139.089 529.009,-131.591 518.482,-132.785 521.524,-139.089\"/>\n",
       "</g>\n",
       "<!-- 10 -->\n",
       "<g id=\"node11\" class=\"node\"><title>10</title>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"891.167,-266 794.167,-266 794.167,-213 891.167,-213 891.167,-266\"/>\n",
       "<text text-anchor=\"start\" x=\"807.667\" y=\"-250.8\" font-family=\"Times,serif\" font-size=\"14.00\">mse = 0.063</text>\n",
       "<text text-anchor=\"start\" x=\"802.167\" y=\"-235.8\" font-family=\"Times,serif\" font-size=\"14.00\">samples = 284</text>\n",
       "<text text-anchor=\"start\" x=\"804.667\" y=\"-220.8\" font-family=\"Times,serif\" font-size=\"14.00\">value = 8.912</text>\n",
       "</g>\n",
       "<!-- 9&#45;&gt;10 -->\n",
       "<g id=\"edge10\" class=\"edge\"><title>9&#45;&gt;10</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M682.251,-227.544C715.529,-230.04 753.357,-232.877 783.81,-235.161\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"783.604,-238.655 793.838,-235.913 784.128,-231.675 783.604,-238.655\"/>\n",
       "</g>\n",
       "<!-- 11 -->\n",
       "<g id=\"node12\" class=\"node\"><title>11</title>\n",
       "<polygon fill=\"#e58139\" fill-opacity=\"0.113725\" stroke=\"black\" points=\"891.167,-195 794.167,-195 794.167,-142 891.167,-142 891.167,-195\"/>\n",
       "<text text-anchor=\"start\" x=\"807.667\" y=\"-179.8\" font-family=\"Times,serif\" font-size=\"14.00\">mse = 0.094</text>\n",
       "<text text-anchor=\"start\" x=\"802.167\" y=\"-164.8\" font-family=\"Times,serif\" font-size=\"14.00\">samples = 619</text>\n",
       "<text text-anchor=\"start\" x=\"804.667\" y=\"-149.8\" font-family=\"Times,serif\" font-size=\"14.00\">value = 9.091</text>\n",
       "</g>\n",
       "<!-- 9&#45;&gt;11 -->\n",
       "<g id=\"edge11\" class=\"edge\"><title>9&#45;&gt;11</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M682.251,-203.704C715.529,-196.355 753.357,-188.002 783.81,-181.277\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"784.828,-184.636 793.838,-179.062 783.318,-177.801 784.828,-184.636\"/>\n",
       "</g>\n",
       "<!-- 13 -->\n",
       "<g id=\"node14\" class=\"node\"><title>13</title>\n",
       "<polygon fill=\"#e58139\" fill-opacity=\"0.392157\" stroke=\"black\" points=\"891.167,-124 794.167,-124 794.167,-71 891.167,-71 891.167,-124\"/>\n",
       "<text text-anchor=\"start\" x=\"807.667\" y=\"-108.8\" font-family=\"Times,serif\" font-size=\"14.00\">mse = 0.049</text>\n",
       "<text text-anchor=\"start\" x=\"802.167\" y=\"-93.8\" font-family=\"Times,serif\" font-size=\"14.00\">samples = 184</text>\n",
       "<text text-anchor=\"start\" x=\"804.667\" y=\"-78.8\" font-family=\"Times,serif\" font-size=\"14.00\">value = 9.539</text>\n",
       "</g>\n",
       "<!-- 12&#45;&gt;13 -->\n",
       "<g id=\"edge13\" class=\"edge\"><title>12&#45;&gt;13</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M676.946,-97.5C711.43,-97.5 751.664,-97.5 783.736,-97.5\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"783.844,-101 793.844,-97.5 783.844,-94.0001 783.844,-101\"/>\n",
       "</g>\n",
       "<!-- 14 -->\n",
       "<g id=\"node15\" class=\"node\"><title>14</title>\n",
       "<polygon fill=\"#e58139\" fill-opacity=\"0.247059\" stroke=\"black\" points=\"891.167,-53 794.167,-53 794.167,-0 891.167,-0 891.167,-53\"/>\n",
       "<text text-anchor=\"start\" x=\"807.667\" y=\"-37.8\" font-family=\"Times,serif\" font-size=\"14.00\">mse = 0.079</text>\n",
       "<text text-anchor=\"start\" x=\"802.167\" y=\"-22.8\" font-family=\"Times,serif\" font-size=\"14.00\">samples = 543</text>\n",
       "<text text-anchor=\"start\" x=\"804.667\" y=\"-7.8\" font-family=\"Times,serif\" font-size=\"14.00\">value = 9.306</text>\n",
       "</g>\n",
       "<!-- 12&#45;&gt;14 -->\n",
       "<g id=\"edge14\" class=\"edge\"><title>12&#45;&gt;14</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M676.946,-75.23C711.579,-64.9842 752.013,-53.0227 784.152,-43.5148\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"785.248,-46.8405 793.844,-40.6474 783.262,-40.1281 785.248,-46.8405\"/>\n",
       "</g>\n",
       "</g>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<graphviz.files.Source at 0x7f2a3f758588>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "draw_tree(model.estimators_[0], df_trn, precision=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have 20k samples at the start of the this tree - because that's what we made the training set as when we split our data.\n",
    "\n",
    "Looking at first node: in our whole dataset (X_train) there're 20k rows, the average sale price is ~10.1, and if we built a model where we used that average all the time: then our MSE would be 0.452. Iow that's the denominator of an R2. This 1st node is the most basic model: a tree with zero splits - just predict the average.\n",
    "\n",
    "The best single split the RF is able to make is based on whether the Coupler_System is ≤ 0.5 (True / False). If it does that, the MSE of Coupler_System > 0.5 (False) goes to 0.114: a large improvement. In the other group: Coupler_System ≤ 0.5 (True) improves slightly to 0.397. The False group is a small fraction: ~1,700/20,000.\n",
    "\n",
    "---\n",
    "\n",
    "**How to find the best possible split with a Single Decision Tree?**:\n",
    "\n",
    "* **For each** categorical variable:\n",
    "    * **For each** value of that variable:\n",
    "        * **Find** the split with the Minimum weighted-average MSE\n",
    "* **Return** the categorry:value split w/ Minimum weighted-average MSE\n",
    "\n",
    "Equivalent to this is to take the MSE of a hypothetical model where all values of a binary split are set to their decisions average.\n",
    "\n",
    "---\n",
    "\n",
    "Now we can improve this Decision Tree by setting `depth=None` to continue until each leaf node has only one decision possible for it. If we do that (surprise) we get a model that perfectly overfits our data. Our validation loss/error is not 1, but is better than our shallow tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5.756054031998178e-17, 0.47752222900820246, 1.0, 0.5927743171457405]\n"
     ]
    }
   ],
   "source": [
    "model = RandomForestRegressor(n_estimators=1, bootstrap=False, n_jobs=-1)\n",
    "model.fit(X_train, y_train)\n",
    "print_score(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Bagging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.4.1 Intro to Bagging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can improve these D.Trees by making forests of them. We create forests with a statistical technique called 'bagging'. Any kind of model can be bagged. A Random Forest is a way of bagging trees.\n",
    "\n",
    "What if we created `N` different models, each of which was only somewhat predictive, but the models weren't at all corelated with eachother. That means the `N` models would had to have found different insights into the relationships in the data. If you took the average of those `N` models, you're effectively bringing in the insights from each of them. This is Ensembling.\n",
    "\n",
    "What if we made a bunch of these big, deep, strongly-overfit trees, but each one only gets a random 1/10th of the data. So each tree will be perfect on that subset but bad at the rest. So each of the trees will be better than nothing, and all overfit in different ways on different things because they use different random samples. \n",
    "\n",
    "So they all have errors, but the errors are random. The average of a bunch of random errors is **Zero**.\n",
    "\n",
    "So if we take the average of all these trees (ea. of which trained on a dfnt rand subset) the errors will average out to zero and what's left is the true relationship. *That* is a **Random Forest**. [Lecture 2](https://youtu.be/blyXCk4sgEg?t=2971)\n",
    "\n",
    "---\n",
    "\n",
    "1. Grab random subset of data.\n",
    "2. Build a Decision Tree on it.\n",
    "3. Put that D.Tree aside and repeat `N` times\n",
    "4. For each D.Tree make predictions by running test data through tree to get to leaf node\n",
    "5. Take average in that leaf node $\\forall$ the trees\n",
    "6. Average them all together.\n",
    "\n",
    "To do that we call `RandomForestRegressor`. An estimator (specfd by `n_estimators`) is a D.Tree.\n",
    "\n",
    "---\n",
    "\n",
    "The key insight is to construct multitple models that are better than nothing and where the errors are as much as possible uncorrelated with eachother. If the errors are correlated this breaks down.\n",
    "\n",
    "For subsets, Scikit-Learn picks out `n` rows *with* replacement. This is called bootstrapping. This on average represents 62.3% of the rows, with a bunch represented multiple times. ***(I think this means 62.3% rows used on any given tree).***\n",
    "\n",
    "[Lecture 2:](https://youtu.be/blyXCk4sgEg?t=3146) So instead of picking out a 1/10 of the rows, of an `n` row dataset, we pick out `n` rows with replacement, which on average represnts 62.3% of the rows, many of them multiple times.\n",
    "\n",
    "*Aside:* The Whole point of modeling Machine Learning is to find a model that tells you which variables are important and how they interact together to drive your independent variable. The difference between using 'Tree Space / Random-Forest Space' and 'Euclidean Space' to find nearest neighbors is the difference between a model that makes good predictions and one that makes meaningless predictions.\n",
    "\n",
    "---\n",
    "\n",
    "In **Bagging** you want each of your individual estimators / trees to be as predictive as possible and for their predictions to be as uncorrelated as possible. The inventor of RFs in the 1990s spoke about this: trying to come up with predictive but poorly-correlated trees.\n",
    "\n",
    "Recent research has shown correlation is more important than individual predictiveness: so recent methods focus on creating trees which are less accurate on their own, and aim to minimize correlation between trees. Scikit-Learn has an ExtraTrees[Regressor/Classifier] with the exact same API as RandomForest[R/C] (and can be dropped in to replace it) which stands for \"Extremely-Randomized Trees Model\" Instead of trying every split of every variable, it randomly tries a few splits of a few variables. It's much faster to train, has much more randomness, and in that time you can build more trees and get better generalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.11125989229028178, 0.3708225122739393, 0.9726051845817276, 0.7544272589887925]\n"
     ]
    }
   ],
   "source": [
    "model = RandomForestRegressor(n_jobs=-1) # default is 10 estimators\n",
    "model.fit(X_train, y_train)\n",
    "print_score(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll grab predictions for each individual tree and look at one example. After you've built a RF, each tree is stored in the attribute: `.estimators_`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([9.51044, 9.15905, 9.82553, 9.21034, 9.3501 , 9.30565, 9.21034, 9.79813, 9.3501 , 9.07681]),\n",
       " 9.379648999374709,\n",
       " 9.104979856318357)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds = np.stack([t.predict(X_valid) for t in model.estimators_])\n",
    "preds[:,0], np.mean(preds[:,0]), y_valid[0] # print first tree's predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 12000)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds.shape # 12,000 predictions for each of 10 trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that most of the predictions were a bit off, but the mean of all of them is pretty good. 9.459 avg, 9.105 actual.\n",
    "\n",
    "Here's a plot going through each of the 10 trees, taking the mean of all the predictions up to the i$^{th}$ tree (1st tree, 1st 2 trees, 1st 3 .. etc), and plot the R$^2$. Notice the final value on the plot matches the R$^2$ value of the model up above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot([metrics.r2_score(y_valid, np.mean(preds[:i+1], axis=0)) for i in range(10)]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also note the plot's flattening out. (tested in original notebook and course nb): adding more trees won't improve the model much beyond this point.\n",
    "\n",
    "The `n_estimators` hyperparameter is chosen based on:\n",
    "1. amount of time you have for fitting\n",
    "2. point of diminshing returns\n",
    "\n",
    "More trees slows model fit/train time, but fewer trees can still offer valuable insight. J.Howard will often work through a day with a 20-tree RF, and at the end expand that to a 1000-tree model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.4.2 Out-of-Bag (OoB) Score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sometimes your dataset will be too small to create a validation set and a good model at the same time. There's a trick unique to Random Forests for this:\n",
    "\n",
    "Recognize that some for each tree, some dataset rows did not get used. So pass those rows through those trees as their validation sets.\n",
    "\n",
    "So you end up with a different validation set for each tree. Now to calculate our prediction we average all the trees where that row was not used for training. As long as you have enough trees every row will appear in the OoB sample for one of them at least.\n",
    "\n",
    "So you create an OoB prediction by averaging all the trees you didn't use to train each individual row, then calculate your RMSE, R2, etc on that.\n",
    "\n",
    "You can do this automatically by specifying the `oob_score=True` parameter in Scikit-Learn's `RandomForestRegressor`, creating a `.oob_score_` attribute in the resulting model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.0969800601698621, 0.3598780368852427, 0.9791859781228975, 0.7687090395771896, 0.8526215860050095]\n"
     ]
    }
   ],
   "source": [
    "model = RandomForestRegressor(n_estimators=40, n_jobs=-1, oob_score=True)\n",
    "model.fit(X_train, y_train)\n",
    "print_score(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The OoB Score will usually slightly underestimate the generalizability of the model -- the more trees you have, the less the underestimation of the model - but it works well enough anyway."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5 Reducing Over-Fitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.5.1 Subsampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the easiest ways to avoid over-fitting is also one of the best ways to speed up analysis: *subsampling*. Let's return to using our full dataset, so we can demonstrate the impact of this technique.\n",
    "\n",
    "___\n",
    "\n",
    "***NOTE***: before we took a subset of 30k rows of the data and built every model on that. Meaning every tree in the RF is a different subset of that subset of 30k. Why not pick a totally different subset of 30k each time. ie: leave the entire dataset of 300k records as is, and if we want to make things faster: pick a different subset of 30k each time. So rather than bootstrapping the entire set of rows: let's just randomly sample a subset of data.\n",
    "\n",
    "---\n",
    "\n",
    "So we'll do this by calling `proc_df` without our subset parameter:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_trn, y_trn, nans = proc_df(df_raw, 'SalePrice')\n",
    "X_train, X_valid = split_vals(df_trn, n_trn)\n",
    "y_train, y_valid = split_vals(y_trn, n_trn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The basic idea is this: rather than limit the total amount of data that our model can access, let's instead limit it to a *different* random subset per tree. That way, given enough trees, the model can still see *all* of the data, but for each individual tree, it'll be just as fast as if we had cut down our dataset as before.\n",
    "\n",
    "Calling `fastai.structurered.set_rf_samples(n)` will change Scikit-learn's random forests to give each tree a random sample of `n` random rows.\n",
    "\n",
    "When we do this, now when we run a RF, it's not going to bootstrap an entire set of 391k rows (len(X_train)), it'll just grab a subset of 20k rows. So when we run `set_rf_samples(20000)` it'll still run just as quickly as if we'd've originally done a random sample of 20k, but now every tree can have access to the entire dataset.\n",
    "\n",
    "So if we use enough D.Trees, the RF will eventually see everything."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_rf_samples(20000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 9.34 s, sys: 468 ms, total: 9.81 s\n",
      "Wall time: 5.08 s\n",
      "[0.23997123416668353, 0.281977920276786, 0.8796480320055804, 0.858003323331318, 0.866939014437051]\n"
     ]
    }
   ],
   "source": [
    "model = RandomForestRegressor(n_jobs=-1, oob_score=True)\n",
    "%time model.fit(X_train, y_train)\n",
    "print_score(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now with 10 estimators (default) we get an R2 of 0.858."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 35.7 s, sys: 1.41 s, total: 37.1 s\n",
      "Wall time: 15.8 s\n",
      "[0.22756958930029839, 0.26205133591808527, 0.8917661075126084, 0.8773632225160998, 0.8801800231758763]\n"
     ]
    }
   ],
   "source": [
    "model = RandomForestRegressor(n_estimators=40, n_jobs=-1, oob_score=True)\n",
    "%time model.fit(X_train, y_train)\n",
    "print_score(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Increasing to 40 esimates increases our R2 score from 0.858 to 0.877.\n",
    "\n",
    "---\n",
    "\n",
    "`set_rf_samples` will be very useful for working with massive structured datasets.\n",
    "\n",
    "Unfortunately as of 31/10/2017 (and now 25/3/2018) it will not change how OoB is calculated (`set_rf_samples` is a hack that replaces scikit-learn's internal function call with a lambda function with the desired behavior). OoB should be turned off for now when `set_rf_samples` is used.\n",
    "\n",
    "***NOTE*** to **reset** the RF sampling: call `fastai.structured.reset_rf_samples()`\n",
    "\n",
    "---\n",
    "\n",
    "When doing EDA (Exploratory Data Analysis) ie: when working and probing at a problem / doing interactive machine learning, [J.Howard](https://youtu.be/blyXCk4sgEg?t=4791) will use `set_rf_samples` (subsets) and reasonable small forests, because:\n",
    "> all the insights that I'm going to get are exactly the same as the big ones, but I can run them in 3 or 4 seconds instead of hours.\n",
    "\n",
    "> this is one of the biggest tips I can give you, and very very few people in industry or academia actually do this. Most people run all of their models on all of the data all of the time using their best possible parameters, which is just pointless.\n",
    "\n",
    "> if you're trying to find out which features are important and how they're related to each other and so forth: having that 4th decimal place of accuracy isn't going to change any of your insights at all.\n",
    "\n",
    "> do most of your models on a large enough sample size that your accuracy is reasonable (w/n a reasonable distance of the best accuracy you can get) and is taking a small number of seconds to train - so you can interactively do your analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.5.2 Tree-building Parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We revert to using a full bootstrap sample in order to show the impact of other over-fitting avoidance methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_rf_samples()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's get a baseline for this full set to compare to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 6min 18s, sys: 988 ms, total: 6min 19s\n",
      "Wall time: 1min 39s\n",
      "[0.07842487866176874, 0.23918781214271065, 0.9871458744654089, 0.8978293710994207, 0.9083933718091257]\n"
     ]
    }
   ],
   "source": [
    "model = RandomForestRegressor(n_estimators=40, n_jobs=-1, oob_score=True)\n",
    "%time model.fit(X_train, y_train)\n",
    "print_score(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each of the estimators will train all the way down until the leaf nodes have 1 sample in them. **NOTE** that our OoB score is better than our validation R2 score (.89278) because our validation set is **not** a random sample: it's a different time period, and it's much harder to predict an entirely different time period than it is to predict random dates.\n",
    "\n",
    "---\n",
    "\n",
    "Another way to reduce over-fitting is to grow our trees less deeply. We do this by specifying (with `min_samples_leaf`) that we require some minimum number of rows in every leafe node. This has 2 benefits:\n",
    "* There are fewer decision rules for each leaf node; simpler models should generalize better\n",
    "* The predictions are made by averaging more rows in the leaf node, resulting in less volatility\n",
    "\n",
    "example: `min_samples_leaf=3`: stop training the tree further when your leaf node has 3 or less samples in it.\n",
    "\n",
    "In practice this means there'll be 1 or 2 fewer levels of decisions being made, which means about half or a quarter the number of actual decision criteria we have to do -- so it'll train quicker. It means also when we look at an individual tree, rather than just taking 1 point, we're taking the average of at least 3 points -- so we expect each tree to generalize a bit better; but ea. tree is also likely to be less powerful on its own."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 5min 22s, sys: 732 ms, total: 5min 23s\n",
      "Wall time: 1min 25s\n",
      "[0.11503700539769621, 0.23330848662578932, 0.9723426884651744, 0.9027904243770585, 0.9085656903600993]\n"
     ]
    }
   ],
   "source": [
    "model = RandomForestRegressor(n_estimators=40, min_samples_leaf=3, n_jobs=-1, oob_score=True)\n",
    "%time model.fit(X_train, y_train)\n",
    "print_score(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Values of **1, 3, 5, 10, 25** tend to work well for `min_samples_leaf`.\n",
    "\n",
    "If working with a massive dataset without subsampling, you may need values of hundreds or thousands.\n",
    "\n",
    "---\n",
    "\n",
    "Here we see increasing `min_samples_leaf` from 1 to 3 has increased our Validation R$^2$ from 0.898 to 0.903. So it's a slight improvement and trains a bit faster.\n",
    "\n",
    "---\n",
    "\n",
    "We can also increase the amount of variation amongst the trees by not only using a sample of rows for each tree, but also using a sample of *columns* for each *split*. We do this by specifying `max_features`, which is the proportion of features to randomly select from at each split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2min 52s, sys: 148 ms, total: 2min 52s\n",
      "Wall time: 47.4 s\n",
      "[0.11924543662523952, 0.22893288409507787, 0.9702820832112764, 0.9064024818437162, 0.9115301725386481]\n"
     ]
    }
   ],
   "source": [
    "model = RandomForestRegressor(n_estimators=40, min_samples_leaf=3, max_features=0.5, n_jobs=-1, oob_score=True)\n",
    "%time model.fit(X_train, y_train)\n",
    "print_score(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our model now has a validation R2 of 0.906. Our RMSE of log(price) has dropped from 0.233 to 0.229 as well. How good is that? Well on the [Kaggle public leaderboard](https://www.kaggle.com/c/bluebook-for-bulldozers/leaderboard) a loss of 0.2289 puts us in the top 20 of the competition. That's with a *\"totally brainless random forest with some totally brainless minor hyperparameter tuning.\"*\n",
    "\n",
    "> This why the Random Forest is such an important - not just first step but often only step in Machine Learning. Because it's hard to screw it up (even when we didn't tune the hyperparameters we still got a good result), and a small amt of hypar tuning got a much better result. \n",
    "\n",
    ">So any kind of model - specifically Linear-type models which have a whole bunch of statistical assumptions and require a bunch of prereqs to get right before they start to work at all - can really throw you off track because they can give you totally wrong answers about how accurate the predictions can be.\n",
    "\n",
    ">The Random Forest generally-speaking tends to work on most datasets most of the time with most sets of hypars.\n",
    "\n",
    "-- [J.Howard Fast.ai ML1 Lecture 2](https://youtu.be/blyXCk4sgEg?t=5370)\n",
    "\n",
    "Random Forests work because the trees are p.much infinitely flexible. Even with a categorical variable - if there're particular categories which have different levels of price: it can gradually zoom in on those groups by using multiple splits.\n",
    "\n",
    "You can help it by telling it the order of your CatVar, but even if you don't: it's okay, it'll just take a few more decisions to get there.\n",
    "\n",
    "In a Linear model, or almost *any* other kind of model, especially non-tree models, encoding  CatVars the way RFs do won't work - because there's no linear relationship between arbitrary identifiers.\n",
    "\n",
    "---\n",
    "\n",
    ">What does `max_features` do? The idea is that the less correlated your trees are w.eachother, the better. Imagine you had 1 column that was so much better than all the others at being predictive, that every single tree you built - regardless of which subset of rows - always started with that column. So the trees will all be pretty similar.\n",
    "\n",
    ">But you can imagine there might be some interaction of variables where that interaction is more important than that individual column. So if every tree always fits on the same thing the 1st time, you're not going to get much variation in those trees.\n",
    "\n",
    ">So what we do is in addition to just taking a subset of rows: we then at every single split point take a different subset of columns.\n",
    "\n",
    ">This is slightly different than row sampling. In row-sampling each new tree is based on a random set of rows. For column sampling every individual binary split we choose from a different subset of columns.\n",
    "\n",
    ">In other words: rather than looking at every possible level of every possible column: we look at every possible level of a random subset of columns. And each binary split / decision point we use a different random subset.\n",
    "\n",
    ">How many? you get to pick. `max_features=0.5` means randomly choose half of them. The default is to use all of them. There are also some special parameters you can pass in (sqrt, log, etc).\n",
    "\n",
    ">In practice I've found good values to be 1, 0.5, log(2), or sqrt -- that'll give you a nice bit of variation.\n",
    "\n",
    "-- [J.Howard Fast.ai ML1 Lecture 2](https://youtu.be/blyXCk4sgEg?t=5049)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "As an example: here's what the Random Forest sees when it's making it's split decisions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0         58\n",
       "1         61\n",
       "2         38\n",
       "3          7\n",
       "4         39\n",
       "5          1\n",
       "6         13\n",
       "7          1\n",
       "8         16\n",
       "9         67\n",
       "10         1\n",
       "11        50\n",
       "12        11\n",
       "13         1\n",
       "14         1\n",
       "15         1\n",
       "16        16\n",
       "17        19\n",
       "18        12\n",
       "19         1\n",
       "20        14\n",
       "21        34\n",
       "22        12\n",
       "23        63\n",
       "24         1\n",
       "25        38\n",
       "26        12\n",
       "27        23\n",
       "28        34\n",
       "29        21\n",
       "          ..\n",
       "401095    16\n",
       "401096    16\n",
       "401097    16\n",
       "401098    16\n",
       "401099    16\n",
       "401100    16\n",
       "401101    16\n",
       "401102    16\n",
       "401103    16\n",
       "401104    16\n",
       "401105    16\n",
       "401106    16\n",
       "401107    16\n",
       "401108    16\n",
       "401109    16\n",
       "401110    16\n",
       "401111    16\n",
       "401112    16\n",
       "401113    16\n",
       "401114    16\n",
       "401115    16\n",
       "401116    16\n",
       "401117    16\n",
       "401118    16\n",
       "401119    12\n",
       "401120    16\n",
       "401121    16\n",
       "401122    16\n",
       "401123    12\n",
       "401124    12\n",
       "Length: 401125, dtype: int8"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_raw.fiProductClassDesc.cat.codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Backhoe Loader - 0.0 to 14.0 Ft Standard Digging Depth',\n",
       "       'Backhoe Loader - 14.0 to 15.0 Ft Standard Digging Depth',\n",
       "       'Backhoe Loader - 15.0 to 16.0 Ft Standard Digging Depth',\n",
       "       'Backhoe Loader - 16.0 + Ft Standard Digging Depth',\n",
       "       'Backhoe Loader - Unidentified',\n",
       "       'Hydraulic Excavator, Track - 0.0 to 2.0 Metric Tons',\n",
       "       'Hydraulic Excavator, Track - 11.0 to 12.0 Metric Tons',\n",
       "       'Hydraulic Excavator, Track - 12.0 to 14.0 Metric Tons',\n",
       "       'Hydraulic Excavator, Track - 14.0 to 16.0 Metric Tons',\n",
       "       'Hydraulic Excavator, Track - 150.0 to 300.0 Metric Tons',\n",
       "       'Hydraulic Excavator, Track - 16.0 to 19.0 Metric Tons',\n",
       "       'Hydraulic Excavator, Track - 19.0 to 21.0 Metric Tons',\n",
       "       'Hydraulic Excavator, Track - 2.0 to 3.0 Metric Tons',\n",
       "       'Hydraulic Excavator, Track - 21.0 to 24.0 Metric Tons',\n",
       "       'Hydraulic Excavator, Track - 24.0 to 28.0 Metric Tons',\n",
       "       'Hydraulic Excavator, Track - 28.0 to 33.0 Metric Tons',\n",
       "       'Hydraulic Excavator, Track - 3.0 to 4.0 Metric Tons',\n",
       "       'Hydraulic Excavator, Track - 300.0 + Metric Tons',\n",
       "       'Hydraulic Excavator, Track - 33.0 to 40.0 Metric Tons',\n",
       "       'Hydraulic Excavator, Track - 4.0 to 5.0 Metric Tons',\n",
       "       'Hydraulic Excavator, Track - 4.0 to 6.0 Metric Tons',\n",
       "       'Hydraulic Excavator, Track - 40.0 to 50.0 Metric Tons',\n",
       "       'Hydraulic Excavator, Track - 5.0 to 6.0 Metric Tons',\n",
       "       'Hydraulic Excavator, Track - 50.0 to 66.0 Metric Tons',\n",
       "       'Hydraulic Excavator, Track - 6.0 to 8.0 Metric Tons',\n",
       "       'Hydraulic Excavator, Track - 66.0 to 90.0 Metric Tons',\n",
       "       'Hydraulic Excavator, Track - 8.0 to 11.0 Metric Tons',\n",
       "       'Hydraulic Excavator, Track - 90.0 to 150.0 Metric Tons',\n",
       "       'Hydraulic Excavator, Track - Unidentified',\n",
       "       'Hydraulic Excavator, Track - Unidentified (Compact Construction)',\n",
       "       'Motorgrader - 130.0 to 145.0 Horsepower',\n",
       "       'Motorgrader - 145.0 to 170.0 Horsepower',\n",
       "       'Motorgrader - 170.0 to 200.0 Horsepower',\n",
       "       'Motorgrader - 200.0 + Horsepower',\n",
       "       'Motorgrader - 45.0 to 130.0 Horsepower', 'Motorgrader - Unidentified',\n",
       "       'Skid Steer Loader - 0.0 to 701.0 Lb Operating Capacity',\n",
       "       'Skid Steer Loader - 1251.0 to 1351.0 Lb Operating Capacity',\n",
       "       'Skid Steer Loader - 1351.0 to 1601.0 Lb Operating Capacity',\n",
       "       'Skid Steer Loader - 1601.0 to 1751.0 Lb Operating Capacity',\n",
       "       'Skid Steer Loader - 1751.0 to 2201.0 Lb Operating Capacity',\n",
       "       'Skid Steer Loader - 2201.0 to 2701.0 Lb Operating Capacity',\n",
       "       'Skid Steer Loader - 2701.0+ Lb Operating Capacity',\n",
       "       'Skid Steer Loader - 701.0 to 976.0 Lb Operating Capacity',\n",
       "       'Skid Steer Loader - 976.0 to 1251.0 Lb Operating Capacity',\n",
       "       'Skid Steer Loader - Unidentified',\n",
       "       'Track Type Tractor, Dozer - 105.0 to 130.0 Horsepower',\n",
       "       'Track Type Tractor, Dozer - 130.0 to 160.0 Horsepower',\n",
       "       'Track Type Tractor, Dozer - 160.0 to 190.0 Horsepower',\n",
       "       'Track Type Tractor, Dozer - 190.0 to 260.0 Horsepower',\n",
       "       'Track Type Tractor, Dozer - 20.0 to 75.0 Horsepower',\n",
       "       'Track Type Tractor, Dozer - 260.0 + Horsepower',\n",
       "       'Track Type Tractor, Dozer - 75.0 to 85.0 Horsepower',\n",
       "       'Track Type Tractor, Dozer - 85.0 to 105.0 Horsepower',\n",
       "       'Track Type Tractor, Dozer - Unidentified',\n",
       "       'Wheel Loader - 0.0 to 40.0 Horsepower',\n",
       "       'Wheel Loader - 100.0 to 110.0 Horsepower',\n",
       "       'Wheel Loader - 1000.0 + Horsepower',\n",
       "       'Wheel Loader - 110.0 to 120.0 Horsepower',\n",
       "       'Wheel Loader - 120.0 to 135.0 Horsepower',\n",
       "       'Wheel Loader - 135.0 to 150.0 Horsepower',\n",
       "       'Wheel Loader - 150.0 to 175.0 Horsepower',\n",
       "       'Wheel Loader - 175.0 to 200.0 Horsepower',\n",
       "       'Wheel Loader - 200.0 to 225.0 Horsepower',\n",
       "       'Wheel Loader - 225.0 to 250.0 Horsepower',\n",
       "       'Wheel Loader - 250.0 to 275.0 Horsepower',\n",
       "       'Wheel Loader - 275.0 to 350.0 Horsepower',\n",
       "       'Wheel Loader - 350.0 to 500.0 Horsepower',\n",
       "       'Wheel Loader - 40.0 to 60.0 Horsepower',\n",
       "       'Wheel Loader - 500.0 to 1000.0 Horsepower',\n",
       "       'Wheel Loader - 60.0 to 80.0 Horsepower',\n",
       "       'Wheel Loader - 80.0 to 90.0 Horsepower',\n",
       "       'Wheel Loader - 90.0 to 100.0 Horsepower',\n",
       "       'Wheel Loader - Unidentified'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_raw.fiProductClassDesc.cat.categories"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
