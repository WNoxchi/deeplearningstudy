{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NB!** The current (30/3/2018) release of PyTorch Torchtext has some bugs preventing execution of this notebook. The updated version of Torchtext used in this notebook can be retrieved via:\n",
    "```\n",
    "pip install --upgrade git+https://github.com/pytorch/text\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pathlib\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path  = pathlib.Path('../../data/')\n",
    "comp  = pathlib.Path('competitions/jigsaw-toxic-comment-classification-challenge/')\n",
    "TRAIN_DATA_FILE = pathlib.Path('train.csv')\n",
    "TEST_DATA_FILE  = pathlib.Path('test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### ~~Fill Missing Values~~\n",
    "\n",
    "(not needed in updated torchtext)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# train = pd.read_csv(path/comp/TRAIN_DATA_FILE)\n",
    "# test  = pd.read_csv(path/comp/TEST_DATA_FILE)\n",
    "\n",
    "# train[\"comment_text\"] = train[\"comment_text\"].fillna(\"_na_\").values\n",
    "# test[\"comment_text\"]  = test[\"comment_text\"].fillna(\"_na_\").values\n",
    "\n",
    "# # update paths to copies on disk\n",
    "# TRAIN_DATA_FILE = pathlib.Path('nafill_' + str(TRAIN_DATA_FILE))\n",
    "# TEST_DATA_FILE  = pathlib.Path('nafill_' + str(TEST_DATA_FILE))\n",
    "\n",
    "# train.to_csv(path/comp/TRAIN_DATA_FILE)\n",
    "# test.to_csv(path/comp/TEST_DATA_FILE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Declare Fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.data import Field\n",
    "\n",
    "tokenize = lambda x: x.split()\n",
    "\n",
    "TEXT = Field(sequential=True, tokenize=tokenize, lower=True)\n",
    "LABEL = Field(sequential=False, use_vocab=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Construct Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.data import TabularDataset\n",
    "    \n",
    "tv_datafields = [(\"id\", None), # we won't be needing the id, so we pass in None as the field\n",
    "                 (\"comment_text\", TEXT), (\"toxic\", LABEL),\n",
    "                 (\"severe_toxic\", LABEL), (\"threat\", LABEL),\n",
    "                 (\"obscene\", LABEL), (\"insult\", LABEL),\n",
    "                 (\"identity_hate\", LABEL)]\n",
    "trn, vld = TabularDataset.splits(\n",
    "               path=\"../../data/competitions/jigsaw-toxic-comment-classification-challenge/\", # the root directory where the data lies\n",
    "               train='train.csv', validation='train.csv',\n",
    "               format='csv',\n",
    "               skip_header=True, # if your csv header has a header, make sure to pass this to ensure it doesn't get proceesed as data!\n",
    "               fields=tv_datafields)\n",
    " \n",
    "tst_datafields = [(\"id\", None), # we won't be needing the id, so we pass in None as the field\n",
    "                  (\"comment_text\", TEXT)]\n",
    "tst = TabularDataset(\n",
    "           path=\"../../data/competitions/jigsaw-toxic-comment-classification-challenge/test.csv\", # the file path\n",
    "           format='csv',\n",
    "           skip_header=True, # if your csv header has a header, make sure to pass this to ensure it doesn't get proceesed as data!\n",
    "           fields=tst_datafields)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torchtext.data.example.Example at 0x7fec3494e518>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trn[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `Example` object bundles the attributes of a single datapoint together. At this point the text has been tokenized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['comment_text', 'toxic', 'severe_toxic', 'threat', 'obscene', 'insult', 'identity_hate'])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trn[0].__dict__.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['comment_text', 'toxic', 'severe_toxic', 'threat', 'obscene', 'insult', 'identity_hate'])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trn[1].__dict__.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['explanation', 'why', 'the']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trn[0].comment_text[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Torchtext handles mapping words to integers, but it has to be told the full range of words it should handle. In our case, we probably want to build the vocabulary on the training set only, so we run the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXT.build_vocab(trn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('the', 490031),\n",
       " ('to', 294069),\n",
       " ('of', 222834),\n",
       " ('and', 218120),\n",
       " ('a', 211778),\n",
       " ('i', 196695),\n",
       " ('you', 187782),\n",
       " ('is', 170753),\n",
       " ('that', 146478),\n",
       " ('in', 140540)]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TEXT.vocab.freqs.most_common(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This makes Torchtext go through all the elements in the training set, check the contents corresponding to the `TEXT` field, and register the words in its vocabulary. Torchtext has its own class called `Vocab` for handling the vocabulary. The `Vocab` class holds a mapping frm word to id in its `stoi` attribute and a reverse mapping in its `itos` attribute. In addition to this, it cn automatically build an embedding matrix for you using various pretrained embeddings like word2vec. The `Vocab` class can also take options like `max_size` and `min_freq` that dictate how many words are in the vocabulary or how many times a word has to appear to be registered in the vocabulary. Words that aren't included in the vocabulary will be converted into `<unk>`, the \"unknown\" token.\n",
    "\n",
    "Now that we have our data formatted and read into memeory, we turn to the next step: creating an Iterator to pass hte data to our model:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Constructing the Iterator\n",
    "\n",
    "'Iterator' is the Torchtext DataLoader with some extra NLP-specific functionalty."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.data import Iterator, BucketIterator\n",
    "\n",
    "device = 0\n",
    "\n",
    "train_iter, val_iter = BucketIterator.splits(\n",
    "    (trn, vld), # we pass in the datasets we want the iterator to draw data from\n",
    "    batch_sizes=(64,64),\n",
    "    device=device, # if you want to use GPU; else: -1\n",
    "    sort_key=lambda x: len(x.comment_text), # BucketIterator needs to be told what function it should use to group the data\n",
    "    sort_within_batch=False,\n",
    "    repeat=False # False bc we want to wrap this Iterator layer\n",
    ")\n",
    "\n",
    "test_iter = Iterator(tst, batch_size=64, device=device, sort=False, \n",
    "                     sort_within_batch=False, repeat=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`BucketIterator` automatically shuffles & buckets input sequences into seqs of similar length. Allows for efficient padding. You *must* tell `BucketIterator` what attribute you want to bucket the data on.\n",
    "\n",
    "Here we want to bucket based on lengths of `comment_text` field. For test data, we don't want to shuffle since we'll be outputting predictions at end fo training -- that's why we use a standard `Iterator`.\n",
    "\n",
    "Output of `BucketIterator`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "[torchtext.data.batch.Batch of size 64]\n",
       "\t[.comment_text]:[torch.cuda.LongTensor of size 65x64 (GPU 0)]\n",
       "\t[.toxic]:[torch.cuda.LongTensor of size 64 (GPU 0)]\n",
       "\t[.severe_toxic]:[torch.cuda.LongTensor of size 64 (GPU 0)]\n",
       "\t[.threat]:[torch.cuda.LongTensor of size 64 (GPU 0)]\n",
       "\t[.obscene]:[torch.cuda.LongTensor of size 64 (GPU 0)]\n",
       "\t[.insult]:[torch.cuda.LongTensor of size 64 (GPU 0)]\n",
       "\t[.identity_hate]:[torch.cuda.LongTensor of size 64 (GPU 0)]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch = next(train_iter.__iter__()); batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The batch has all the fields we passed to the Dataset as attributes. The batch data can be accessed through the attribute with the same name:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['batch_size', 'dataset', 'train', 'fields', 'comment_text', 'toxic', 'severe_toxic', 'threat', 'obscene', 'insult', 'identity_hate'])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch.__dict__.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wrapping the Iterator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `Iterator` returns a custom datatype: `torch.data.Batch` `Batch` has similar API to `Example`: w/ a batch of data from each field as attributes. We can use a simple wrapper to ease use. $\\longrightarrow$ convert `Batch` to tuple: (x, y):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BatchWrapper:\n",
    "    def __init__(self, dl, x, y):\n",
    "        self.dl, self.x, self.y = dl, x, y\n",
    "    \n",
    "    def __iter__(self):\n",
    "        x = getattr(batch, self.x) # assuming only 1 input in this wrapper\n",
    "        \n",
    "        if self.y is not None: # we'll concat y into a single tensor\n",
    "            y = torch.cat([getattr(batch, feat).unsqueeze(1) for feat in self.y], dim=1).float()\n",
    "        else:\n",
    "            y = torch.zeros((1))\n",
    "            \n",
    "        yield (x, y)\n",
    "            \n",
    "    def __len__(self):\n",
    "        return len(self.dl)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dl = BatchWrapper(train_iter, \"comment_text\", list(trn.fields.keys())[2:])\n",
    "\n",
    "valid_dl = BatchWrapper(train_iter, \"comment_text\", list(trn.fields.keys())[2:])\n",
    "\n",
    "test_dl  = BatchWrapper(test_iter, \"comment_text\", None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['id', 'comment_text', 'toxic', 'severe_toxic', 'threat', 'obscene', 'insult', 'identity_hate'])\n",
      "['toxic', 'severe_toxic', 'threat', 'obscene', 'insult', 'identity_hate']\n"
     ]
    }
   ],
   "source": [
    "print(trn.fields.keys())\n",
    "print(list(trn.fields.keys())[2:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All we're doing here is converting the `Batch` object into a tuple of inputs and outputs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Variable containing:\n",
       "  1.1000e+02  1.7900e+02  1.8000e+01  ...   1.8000e+01  1.8000e+01  1.8000e+01\n",
       "  5.6600e+02  7.4700e+02  1.5048e+05  ...   1.9000e+01  4.1830e+03  7.0000e+00\n",
       "  1.1000e+02  7.9000e+01  7.5000e+01  ...   4.5290e+03  1.6000e+01  2.0000e+01\n",
       "                 ...                   â‹±                   ...                \n",
       "  1.1121e+05  8.2436e+04  1.8000e+01  ...   1.3300e+02  3.4210e+03  4.0000e+00\n",
       "  1.0000e+00  1.0000e+00  1.0000e+00  ...   4.0600e+02  2.2240e+05  1.7248e+05\n",
       "  1.0000e+00  1.0000e+00  1.0000e+00  ...   1.8000e+01  1.8000e+01  1.8000e+01\n",
       " [torch.cuda.LongTensor of size 65x64 (GPU 0)], Variable containing:\n",
       "     0     0     0     0     0     0\n",
       "     0     0     0     0     0     0\n",
       "     0     0     0     0     0     0\n",
       "     0     0     0     0     0     0\n",
       "     0     0     0     0     0     0\n",
       "     0     0     0     0     0     0\n",
       "     0     0     0     0     0     0\n",
       "     0     0     0     0     0     0\n",
       "     0     0     0     0     0     0\n",
       "     1     0     1     0     1     0\n",
       "     0     0     0     0     0     0\n",
       "     0     0     0     0     0     0\n",
       "     0     0     0     0     0     0\n",
       "     0     0     0     0     0     0\n",
       "     0     0     0     0     0     0\n",
       "     0     0     0     0     0     0\n",
       "     0     0     0     0     0     0\n",
       "     0     0     0     0     0     0\n",
       "     0     0     0     0     0     0\n",
       "     0     0     0     0     0     0\n",
       "     0     0     0     0     0     0\n",
       "     0     0     0     0     0     0\n",
       "     0     0     0     0     0     0\n",
       "     0     0     0     0     0     0\n",
       "     0     0     0     0     0     0\n",
       "     0     0     0     0     0     0\n",
       "     0     0     0     0     0     0\n",
       "     0     0     0     0     0     0\n",
       "     0     0     0     0     0     0\n",
       "     0     0     0     0     0     0\n",
       "     0     0     0     0     0     0\n",
       "     0     0     0     0     0     0\n",
       "     0     0     0     0     0     0\n",
       "     0     0     0     0     0     0\n",
       "     0     0     0     0     0     0\n",
       "     0     0     0     0     0     0\n",
       "     0     0     0     0     0     0\n",
       "     0     0     0     0     0     0\n",
       "     0     0     0     0     0     0\n",
       "     0     0     0     0     0     0\n",
       "     0     0     0     0     0     0\n",
       "     0     0     0     0     0     0\n",
       "     1     0     1     0     0     0\n",
       "     0     0     0     0     0     0\n",
       "     0     0     0     0     0     0\n",
       "     0     0     0     0     0     0\n",
       "     0     0     0     0     0     0\n",
       "     0     0     0     0     0     0\n",
       "     0     0     0     0     0     0\n",
       "     1     0     0     0     0     0\n",
       "     1     0     0     0     0     0\n",
       "     0     0     0     0     0     0\n",
       "     0     0     0     0     0     0\n",
       "     0     0     0     0     0     0\n",
       "     1     0     1     0     1     0\n",
       "     0     0     0     0     0     0\n",
       "     1     0     0     0     0     0\n",
       "     0     0     0     0     0     0\n",
       "     0     0     0     0     0     0\n",
       "     0     0     0     0     0     0\n",
       "     0     0     0     0     0     0\n",
       "     0     0     0     0     0     0\n",
       "     0     0     0     0     0     0\n",
       "     0     0     0     0     0     0\n",
       " [torch.cuda.FloatTensor of size 64x6 (GPU 0)])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(train_dl.__iter__())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we're ready to start training a model!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training a Text Classifier -- LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll use a simple LSTM as a baseline example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleBiLSTMBaseline(nn.Module):\n",
    "    def __init__(self, hidden_dim, emb_dim=300, \n",
    "                 spatial_dropout=0.05, recurrent_dropout=0.1, num_linear=1):\n",
    "        super().__init__() # don't forget to call this\n",
    "        self.embedding = nn.Embedding(len(TEXT.vocab), emb_dim)\n",
    "        self.encoder = nn.LSTM(emb_dim, hidden_dim, num_layers=1, dropout=recurrent_dropout)\n",
    "        self.linear_layers = []\n",
    "        for i in range(num_linear - 1):\n",
    "            self.linear_layers.append(nn.Linear(hidden_dim, hidden_dim))\n",
    "        self.linear_layers = nn.ModuleList(self.linear_layers)\n",
    "        self.predictor = nn.Linear(hidden_dim, 6)\n",
    "    \n",
    "    def forward(self, seq):\n",
    "        hdn, _ = self.encoder(self.embedding(seq))\n",
    "        feature = hdn[-1, :, :]\n",
    "        for layer in self.linear_layers:\n",
    "            feature = layer(feature)\n",
    "        preds = self.predictor(feature)\n",
    "        return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_sz = 300\n",
    "nh     = 500\n",
    "nl     = 3\n",
    "model  = SimpleBiLSTMBaseline(nh, emb_dim=emb_sz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SimpleBiLSTMBaseline(\n",
       "  (embedding): Embedding(470342, 300)\n",
       "  (encoder): LSTM(300, 500, dropout=0.1)\n",
       "  (linear_layers): ModuleList(\n",
       "  )\n",
       "  (predictor): Linear(in_features=500, out_features=6, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Loop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can iterate using our wrapped `Iterator`, and the data will automatically be passed to us after being moved to the GPU and numericalized appropriately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm\n",
    "\n",
    "opt = torch.optim.Adam(model.parameters(), lr=1e-2)\n",
    "loss_fn = torch.nn.BCEWithLogitsLoss()\n",
    "\n",
    "epochs = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 1/2494 [00:00<06:45,  6.15it/s]\n",
      "  0%|          | 1/2494 [00:00<06:06,  6.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Training Loss: 0.0001, Validation Loss: 0.0000\n",
      "Epoch: 2, Training Loss: 0.0000, Validation Loss: 0.0000\n",
      "CPU times: user 232 ms, sys: 124 ms, total: 356 ms\n",
      "Wall time: 352 ms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for epoch in range(1, epochs+1):\n",
    "    running_loss = 0.0\n",
    "    running_corrects = 0\n",
    "    model.train() # turn on training mode\n",
    "    for x, y in tqdm.tqdm(train_dl): # thanks to our wrapper we can intuitively iterate over our data\n",
    "        opt.zero_grad()\n",
    "        preds = model(x)\n",
    "        loss = loss_fn(preds, y)\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        \n",
    "        running_loss += loss.data[0] * x.size(0)\n",
    "        \n",
    "    epoch_loss = running_loss / len(trn)\n",
    "    \n",
    "    # calculate the vlaidation loss for this epoch\n",
    "    val_loss = 0.0\n",
    "    model.eval() # turn on evaluation mode\n",
    "    for x, y in valid_dl:\n",
    "        preds = model(x)\n",
    "        loss = loss_fn(preds, y)\n",
    "        val_loss += loss.data[0] * x.size(0)\n",
    "        \n",
    "    val_loss /= len(vld)\n",
    "    print(f'Epoch: {epoch}, Training Loss: {epoch_loss:.4f}, Validation Loss: {val_loss:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleBiLSTMBaseline(nn.Module):\n",
    "    def __init__(self, hidden_dim, emb_dim=300,\n",
    "                 spatial_dropout=0.05, recurrent_dropout=0.1, num_linear=1):\n",
    "        super().__init__() # don't forget to call this!\n",
    "        self.embedding = nn.Embedding(len(TEXT.vocab), emb_dim)\n",
    "        self.encoder = nn.LSTM(emb_dim, hidden_dim, num_layers=1, dropout=recurrent_dropout)\n",
    "        self.linear_layers = []\n",
    "        for _ in range(num_linear - 1):\n",
    "            self.linear_layers.append(nn.Linear(hidden_dim, hidden_dim))\n",
    "        self.linear_layers = nn.ModuleList(self.linear_layers)\n",
    "        self.predictor = nn.Linear(hidden_dim, 6)\n",
    "    \n",
    "    def forward(self, seq):\n",
    "        hdn, _ = self.encoder(self.embedding(seq))\n",
    "        feature = hdn[-1, :, :]\n",
    "        for layer in self.linear_layers:\n",
    "            feature = layer(feature)\n",
    "        preds = self.predictor(feature)\n",
    "        return preds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SimpleBiLSTMBaseline(\n",
       "  (embedding): Embedding(470342, 100)\n",
       "  (encoder): LSTM(100, 500, dropout=0.1)\n",
       "  (linear_layers): ModuleList(\n",
       "  )\n",
       "  (predictor): Linear(in_features=500, out_features=6, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "em_sz = 100\n",
    "nh = 500\n",
    "nl = 3\n",
    "model = SimpleBiLSTMBaseline(nh, emb_dim=em_sz); model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 1/2494 [00:00<03:37, 11.46it/s]\n",
      "  0%|          | 1/2494 [00:00<03:33, 11.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Training Loss: 0.0003, Validation Loss: 0.0001\n",
      "Epoch: 2, Training Loss: 0.0001, Validation Loss: 0.0001\n",
      "CPU times: user 156 ms, sys: 60 ms, total: 216 ms\n",
      "Wall time: 212 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "\n",
    "opt = optim.Adam(model.parameters(), lr=1e-2)\n",
    "loss_func = nn.BCEWithLogitsLoss()\n",
    "\n",
    "epochs = 2\n",
    "\n",
    "for epoch in range(1, epochs + 1):\n",
    "    running_loss = 0.0\n",
    "    running_corrects = 0\n",
    "    model.train() # turn on training mode\n",
    "    for x, y in tqdm.tqdm(train_dl): # thanks to our wrapper, we can intuitively iterate over our data!\n",
    "        opt.zero_grad()\n",
    "\n",
    "        preds = model(x)\n",
    "        loss = loss_func(preds, y)\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        \n",
    "        running_loss += loss.data[0] * x.size(0)\n",
    "        \n",
    "    epoch_loss = running_loss / len(trn)\n",
    "    \n",
    "    # calculate the validation loss for this epoch\n",
    "    val_loss = 0.0\n",
    "    model.eval() # turn on evaluation mode\n",
    "    for x, y in valid_dl:\n",
    "        preds = model(x)\n",
    "        loss = loss_func(preds, y)\n",
    "        val_loss += loss.data[0] * x.size(0)\n",
    "\n",
    "    val_loss /= len(vld)\n",
    "    print('Epoch: {}, Training Loss: {:.4f}, Validation Loss: {:.4f}'.format(epoch, epoch_loss, val_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
